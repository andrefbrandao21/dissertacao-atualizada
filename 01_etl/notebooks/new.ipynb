{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c656a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO ETL MEI OTIMIZADO ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "沒 K3241.K03200Y0.D50913.ESTABELE: 13it [19:51, 91.67s/it] \n",
      "沒 K3241.K03200Y1.D50913.ESTABELE: 3it [15:58, 319.50s/it]\n",
      "沒 K3241.K03200Y2.D50913.ESTABELE: 3it [17:05, 342.00s/it]\n",
      "沒 K3241.K03200Y3.D50913.ESTABELE: 3it [17:37, 352.41s/it]\n",
      "沒 K3241.K03200Y4.D50913.ESTABELE: 3it [00:29,  9.75s/it]\n",
      "沒 K3241.K03200Y5.D50913.ESTABELE: 3it [00:25,  8.51s/it]\n",
      "沒 K3241.K03200Y6.D50913.ESTABELE: 3it [17:24, 348.11s/it]\n",
      "沒 K3241.K03200Y7.D50913.ESTABELE: 3it [16:02, 320.68s/it]\n",
      "沒 K3241.K03200Y8.D50913.ESTABELE: 3it [15:02, 300.99s/it]\n",
      "沒 K3241.K03200Y9.D50913.ESTABELE: 3it [00:55, 18.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "笨 Painel concluﾃｭdo: ../data/processed/painel_mei_rf_anual.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# --- CONFIGURAﾃﾃグ ---\n",
    "BASE_PATH = \"../data/raw/CNPJ\"\n",
    "ARQUIVOS_ESTAB = [os.path.join(BASE_PATH, f\"Estabelecimentos{i}/K3241.K03200Y{i}.D50913.ESTABELE\") for i in range(10)]\n",
    "CAMINHO_SIMPLES = os.path.join(BASE_PATH, \"Simples/F.K03200$W.SIMPLES.CSV.D50913\")\n",
    "SAIDA_FINAL = \"../data/processed/painel_mei_rf_anual.parquet\"\n",
    "\n",
    "def main():\n",
    "    print(\"--- INICIANDO ETL MEI OTIMIZADO ---\")\n",
    "    \n",
    "    # 1. Carregar Simples com tipos otimizados\n",
    "    # O CNPJ Bﾃ｡sico deve ser string ou int para o merge bater\n",
    "    df_simples = pd.read_csv(CAMINHO_SIMPLES, sep=\";\", encoding=\"latin-1\", header=None, \n",
    "                             usecols=[0, 4, 5, 6], names=[\"cnpj_basico\", \"mei\", \"ini\", \"fim\"],\n",
    "                             dtype={\"cnpj_basico\": \"int32\", \"mei\": \"category\"})\n",
    "    \n",
    "    df_simples = df_simples[df_simples[\"mei\"] == \"S\"].copy()\n",
    "    \n",
    "    # Datas para Ano (Vetorizado) - convertemos para int16 para economizar RAM\n",
    "    df_simples['ano_ini'] = (df_simples['ini'] // 10000).fillna(0).astype(\"int16\")\n",
    "    df_simples['ano_fim'] = (df_simples['fim'] // 10000).fillna(0).astype(\"int16\")\n",
    "    df_simples.drop(columns=[\"mei\", \"ini\", \"fim\"], inplace=True)\n",
    "\n",
    "    painel_final_lista = []\n",
    "\n",
    "    # 2. Loop Estabelecimentos\n",
    "    for path in ARQUIVOS_ESTAB:\n",
    "        if not os.path.exists(path): continue\n",
    "        \n",
    "        # Lista temporﾃ｡ria por ARQUIVO para evitar fragmentaﾃｧﾃ｣o global\n",
    "        painel_arquivo = []\n",
    "        \n",
    "        reader = pd.read_csv(path, sep=\";\", encoding=\"latin-1\", header=None,\n",
    "                             usecols=[0, 11, 20], names=[\"cnpj_basico\", \"cnae\", \"mun\"],\n",
    "                             dtype={\"cnpj_basico\": \"int32\", \"cnae\": \"float64\", \"mun\": \"float64\"},\n",
    "                             chunksize=2_000_000)\n",
    "        \n",
    "        for chunk in tqdm(reader, desc=f\"沒 {os.path.basename(path)}\"):\n",
    "            # Merge Inner (remove quem nﾃ｣o ﾃｩ MEI logo de cara)\n",
    "            chunk = chunk.merge(df_simples, on=\"cnpj_basico\", how=\"inner\")\n",
    "            \n",
    "            # Setor Vetorizado (Muito mais rﾃ｡pido que .apply)\n",
    "            # CNAE na RF costuma ter 7 dﾃｭgitos. Divisﾃ｣o sﾃ｣o os 2 primeiros.\n",
    "            divisao = (chunk['cnae'] // 100000).fillna(0).astype(int)\n",
    "            chunk['setor'] = 'Servicos'\n",
    "            chunk.loc[divisao.between(1, 3), 'setor'] = 'Agro'\n",
    "            chunk.loc[divisao.between(5, 33), 'setor'] = 'Industria'\n",
    "            chunk.loc[divisao == 84, 'setor'] = 'Setor Publico'\n",
    "            \n",
    "            # Agregaﾃｧﾃ｣o rﾃ｡pida\n",
    "            ent = chunk.groupby(['mun', 'setor', 'ano_ini']).size().reset_index(name='entradas')\n",
    "            sai = chunk.dropna(subset=['ano_fim']).groupby(['mun', 'setor', 'ano_fim']).size().reset_index(name='saidas')\n",
    "            \n",
    "            painel_arquivo.append(pd.merge(ent.rename(columns={'ano_ini': 'ano'}), \n",
    "                                           sai.rename(columns={'ano_fim': 'ano'}), \n",
    "                                           on=['mun', 'setor', 'ano'], how='outer').fillna(0))\n",
    "            \n",
    "        # Consolida o arquivo atual e limpa a RAM antes do prﾃｳximo\n",
    "        if painel_arquivo:\n",
    "            df_arq = pd.concat(painel_arquivo).groupby(['mun', 'setor', 'ano']).sum().reset_index()\n",
    "            painel_final_lista.append(df_arq)\n",
    "            del painel_arquivo; gc.collect()\n",
    "\n",
    "    # 3. Consolidaﾃｧﾃ｣o e Cﾃ｡lculo de Estoque\n",
    "    df_final = pd.concat(painel_final_lista).groupby(['mun', 'setor', 'ano']).sum().reset_index()\n",
    "    \n",
    "    # Ordenar para o cumsum fazer sentido (Econometria de Painel)\n",
    "    df_final = df_final.sort_values(['mun', 'setor', 'ano'])\n",
    "    df_final['estoque_mei'] = (\n",
    "        df_final.groupby(['mun', 'setor'])['entradas'].cumsum() - \n",
    "        df_final.groupby(['mun', 'setor'])['saidas'].cumsum()\n",
    "    )\n",
    "    df_final.to_parquet(SAIDA_FINAL, index=False)\n",
    "    print(f\"笨 Painel concluﾃｭdo: {SAIDA_FINAL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68225c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO ETL MEI OTIMIZADO E CORRIGIDO ---\n",
      "泅 Carregando base do Simples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "沒 K3241.K03200Y0.D50913.ESTABELE: 13it [02:33, 11.80s/it]\n",
      "沒 K3241.K03200Y1.D50913.ESTABELE: 3it [01:05, 21.98s/it]\n",
      "沒 K3241.K03200Y2.D50913.ESTABELE: 3it [01:04, 21.38s/it]\n",
      "沒 K3241.K03200Y3.D50913.ESTABELE: 3it [01:08, 22.73s/it]\n",
      "沒 K3241.K03200Y4.D50913.ESTABELE: 3it [00:58, 19.61s/it]\n",
      "沒 K3241.K03200Y5.D50913.ESTABELE: 3it [00:46, 15.44s/it]\n",
      "沒 K3241.K03200Y6.D50913.ESTABELE: 3it [00:48, 16.11s/it]\n",
      "沒 K3241.K03200Y7.D50913.ESTABELE: 3it [00:49, 16.62s/it]\n",
      "沒 K3241.K03200Y8.D50913.ESTABELE: 3it [00:47, 15.92s/it]\n",
      "沒 K3241.K03200Y9.D50913.ESTABELE: 3it [00:48, 16.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "沒 Gerando painel final e calculando estoques...\n",
      "笨 Painel concluﾃｭdo com sucesso: ../data/processed\\painel_mei_rf_anual.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURAﾃﾃグ DE CAMINHOS ---\n",
    "BASE_PATH = \"../data/raw/CNPJ\"\n",
    "# Lista os 10 arquivos de estabelecimentos conforme o layout da RFB [cite: 3, 16]\n",
    "ARQUIVOS_ESTAB = [os.path.join(BASE_PATH, f\"Estabelecimentos{i}/K3241.K03200Y{i}.D50913.ESTABELE\") for i in range(10)]\n",
    "CAMINHO_SIMPLES = os.path.join(BASE_PATH, \"Simples/F.K03200$W.SIMPLES.CSV.D50913\")\n",
    "OUTPUT_DIR = \"../data/processed\"\n",
    "SAIDA_FINAL = os.path.join(OUTPUT_DIR, 'painel_mei_rf_anual.parquet')\n",
    "\n",
    "def main():\n",
    "    print(\"--- INICIANDO ETL MEI OTIMIZADO E CORRIGIDO ---\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # 1. Carregar Simples: Contﾃｩm os indicadores de opﾃｧﾃ｣o pelo MEI e as datas [cite: 20]\n",
    "    print(\"泅 Carregando base do Simples...\")\n",
    "    df_simples = pd.read_csv(\n",
    "        CAMINHO_SIMPLES, \n",
    "        sep=\";\", \n",
    "        encoding=\"latin-1\", \n",
    "        header=None, \n",
    "        usecols=[0, 4, 5, 6], \n",
    "        names=[\"cnpj_basico\", \"opcao_mei\", \"data_ini\", \"data_fim\"],\n",
    "        dtype={\"cnpj_basico\": \"int32\", \"opcao_mei\": \"str\", \"data_ini\": \"str\", \"data_fim\": \"str\"}\n",
    "    )\n",
    "    \n",
    "    # Filtra apenas quem ﾃｩ ou jﾃ｡ foi optante pelo MEI [cite: 20]\n",
    "    df_simples = df_simples[df_simples[\"opcao_mei\"] == \"S\"].copy()\n",
    "    \n",
    "    # Converte datas para Ano via vetorizaﾃｧﾃ｣o (YYYYMMDD -> YYYY) \n",
    "    df_simples['ano_ini'] = pd.to_numeric(df_simples['data_ini'], errors='coerce').fillna(0) // 10000\n",
    "    df_simples['ano_fim'] = pd.to_numeric(df_simples['data_fim'], errors='coerce').fillna(0) // 10000\n",
    "    \n",
    "    # Limpeza: Mantemos apenas anos razoﾃ｡veis para evitar o \"ano zero\"\n",
    "    df_simples = df_simples.drop(columns=[\"opcao_mei\", \"data_ini\", \"data_fim\"])\n",
    "    gc.collect()\n",
    "\n",
    "    painel_final_lista = []\n",
    "\n",
    "    # 2. Loop Estabelecimentos: Contﾃｩm CNAE e Cﾃｳdigo de Municﾃｭpio [cite: 3, 17]\n",
    "    for path in ARQUIVOS_ESTAB:\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "        \n",
    "        painel_arquivo = []\n",
    "        # Chunksize grande para velocidade, mas monitorando a RAM [cite: 38, 39]\n",
    "        reader = pd.read_csv(\n",
    "            path, \n",
    "            sep=\";\", \n",
    "            encoding=\"latin-1\", \n",
    "            header=None,\n",
    "            usecols=[0, 11, 20], \n",
    "            names=[\"cnpj_basico\", \"cnae\", \"mun\"],\n",
    "            dtype={\"cnpj_basico\": \"int32\", \"cnae\": \"str\", \"mun\": \"float64\"},\n",
    "            chunksize=2_000_000\n",
    "        )\n",
    "        \n",
    "        for chunk in tqdm(reader, desc=f\"沒 {os.path.basename(path)}\"):\n",
    "            # Cruzamento interno: sﾃｳ processamos quem estﾃ｡ na lista de MEIs [cite: 5, 11]\n",
    "            chunk = chunk.merge(df_simples, on=\"cnpj_basico\", how=\"inner\")\n",
    "            \n",
    "            # Mapeamento Setorial (Divisﾃ｣o CNAE - 2 primeiros dﾃｭgitos) [cite: 17, 36]\n",
    "            div = chunk['cnae'].str[:2].fillna('00')\n",
    "            chunk['setor'] = 'Servicos'\n",
    "            chunk.loc[div.isin(['01', '02', '03']), 'setor'] = 'Agro'\n",
    "            chunk.loc[div.astype(int).between(5, 33), 'setor'] = 'Industria'\n",
    "            chunk.loc[div == '84', 'setor'] = 'Setor Publico'\n",
    "            \n",
    "            # Agregaﾃｧﾃ｣o de Entradas: Apenas anos vﾃ｡lidos (> 1900)\n",
    "            ent = chunk[chunk['ano_ini'] > 1900].groupby(['mun', 'setor', 'ano_ini']).size().reset_index(name='entradas')\n",
    "            ent.columns = ['mun', 'setor', 'ano', 'entradas']\n",
    "            \n",
    "            # Agregaﾃｧﾃ｣o de Saﾃｭdas: CRUCIAL - Ignora NaTs/Zeros para evitar estoque negativo\n",
    "            sai = chunk[chunk['ano_fim'] > 1900].groupby(['mun', 'setor', 'ano_fim']).size().reset_index(name='saidas')\n",
    "            sai.columns = ['mun', 'setor', 'ano', 'saidas']\n",
    "            \n",
    "            # Merge de Fluxo\n",
    "            fluxo = pd.merge(ent, sai, on=['mun', 'setor', 'ano'], how='outer').fillna(0)\n",
    "            painel_arquivo.append(fluxo)\n",
    "            \n",
    "            del chunk, ent, sai, fluxo\n",
    "            gc.collect()\n",
    "            \n",
    "        if painel_arquivo:\n",
    "            # Consolida o arquivo atual antes de ir para o prﾃｳximo (previne fragmentaﾃｧﾃ｣o)\n",
    "            df_arq = pd.concat(painel_arquivo).groupby(['mun', 'setor', 'ano']).sum().reset_index()\n",
    "            painel_final_lista.append(df_arq)\n",
    "            del painel_arquivo\n",
    "            gc.collect()\n",
    "\n",
    "    # 3. Consolidaﾃｧﾃ｣o e Cﾃ｡lculo de Estoque\n",
    "    print(\"\\n沒 Gerando painel final e calculando estoques...\")\n",
    "    df_final = pd.concat(painel_final_lista).groupby(['mun', 'setor', 'ano']).sum().reset_index()\n",
    "    \n",
    "    # Ordenaﾃｧﾃ｣o necessﾃ｡ria para o cﾃ｡lculo acumulado (Time-series)\n",
    "    df_final = df_final.sort_values(['mun', 'setor', 'ano'])\n",
    "    \n",
    "    # Estoque = Soma acumulada de quem entrou (-) soma acumulada de quem saiu\n",
    "    df_final['estoque_mei'] = (\n",
    "        df_final.groupby(['mun', 'setor'])['entradas'].cumsum() - \n",
    "        df_final.groupby(['mun', 'setor'])['saidas'].cumsum()\n",
    "    )\n",
    "\n",
    "    # Exportaﾃｧﾃ｣o em Parquet para performance no modelo economﾃｩtrico\n",
    "    df_final.to_parquet(SAIDA_FINAL, index=False)\n",
    "    print(f\"笨 Painel concluﾃｭdo com sucesso: {SAIDA_FINAL}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6dd0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# --- CONFIGURAﾃﾃグ DE CAMINHOS ---\n",
    "INPUT_FILE = '../data/raw/test.parquet'\n",
    "OUTPUT_DIR = '../data/processed'\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, 'rais_painel_balanceado.parquet')\n",
    "\n",
    "def processar_rais_otimizado(path):\n",
    "    \"\"\"\n",
    "    Lﾃｪ apenas colunas necessﾃ｡rias e usa vetorizaﾃｧﾃ｣o para economizar RAM.\n",
    "    \"\"\"\n",
    "    print(\"   [1/3] Lendo colunas selecionadas e mapeando setores...\")\n",
    "    \n",
    "    # Lﾃｪ apenas o necessﾃ｡rio\n",
    "    cols = ['id_municipio', 'cnae_2', 'ano', 'quantidade_vinculos_ativos']\n",
    "    df = pd.read_parquet(path, columns=cols)\n",
    "\n",
    "    # Vetorizaﾃｧﾃ｣o em vez de apply: muito mais rﾃ｡pido e leve\n",
    "    # CNAE Divisﾃ｣o (2 dﾃｭgitos)\n",
    "    div = (df['cnae_2'].fillna(0).astype(np.int32) // 1000)\n",
    "    \n",
    "    df['setor'] = 'Outros'\n",
    "    df.loc[div.between(1, 3), 'setor'] = 'Agro'\n",
    "    df.loc[div.between(5, 33), 'setor'] = 'Industria'\n",
    "    df.loc[div == 84, 'setor'] = 'Setor Publico'\n",
    "    df.loc[div.between(35, 99), 'setor'] = 'Servicos'\n",
    "    \n",
    "    # Remove coluna temporﾃ｡ria e libera memﾃｳria\n",
    "    df.drop(columns=['cnae_2'], inplace=True)\n",
    "\n",
    "    print(\"   [2/3] Agregando estoque...\")\n",
    "    df_agg = (df.groupby(['id_municipio', 'setor', 'ano'], as_index=False)\n",
    "                ['quantidade_vinculos_ativos'].sum())\n",
    "    \n",
    "    del df\n",
    "    gc.collect()\n",
    "    return df_agg\n",
    "\n",
    "def rebalancear_painel_otimizado(df_agg):\n",
    "    \"\"\"\n",
    "    Usa tipos de dados categﾃｳricos para reduzir o tamanho do produto cartesiano.\n",
    "    \"\"\"\n",
    "    print(\"   [3/3] Rebalanceando o painel...\")\n",
    "\n",
    "    # Remover 'Outros' antes de balancear economiza memﾃｳria\n",
    "    df_agg = df_agg[df_agg['setor'] != 'Outros'].copy()\n",
    "\n",
    "    todos_municipios = df_agg['id_municipio'].unique()\n",
    "    todos_setores = ['Agro', 'Industria', 'Servicos', 'Setor Publico']\n",
    "    todos_anos = df_agg['ano'].unique()\n",
    "\n",
    "    # Criar index e esqueleto\n",
    "    index = pd.MultiIndex.from_product(\n",
    "        [todos_municipios, todos_setores, todos_anos], \n",
    "        names=['id_municipio', 'setor', 'ano']\n",
    "    )\n",
    "    \n",
    "    # Reindex ﾃｩ mais eficiente que Merge para balanceamento\n",
    "    df_final = (df_agg.set_index(['id_municipio', 'setor', 'ano'])\n",
    "                      .reindex(index, fill_value=0)\n",
    "                      .reset_index())\n",
    "\n",
    "    # Variﾃ｡vel para DID\n",
    "    df_final['log_estoque'] = np.log1p(df_final['quantidade_vinculos_ativos'].astype(np.float32))\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def main():\n",
    "    print(\"--- INICIANDO ETL RAIS OTIMIZADO ---\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"ERRO: Arquivo de entrada nﾃ｣o encontrado: {INPUT_FILE}\")\n",
    "        return\n",
    "\n",
    "    # Processamento em etapas com limpeza de cache\n",
    "    df_agg = processar_rais_otimizado(INPUT_FILE)\n",
    "    df_final = rebalancear_painel_otimizado(df_agg)\n",
    "    \n",
    "    print(f\"Salvando em: {OUTPUT_FILE}\")\n",
    "    # compressﾃ｣o snappy ﾃｩ padrﾃ｣o e rﾃ｡pida\n",
    "    df_final.to_parquet(OUTPUT_FILE, index=False, compression='snappy')\n",
    "    print(\"--- CONCLUﾃ好O ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ed66446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_simples_base(path, dtypes):\n",
    "    df = pd.read_csv(path, sep=\";\", encoding=\"latin-1\", header=None, \n",
    "                     usecols=[0, 4, 5, 6], names=[\"cnpj_basico\", \"mei\", \"ini\", \"fim\"],\n",
    "                     dtype=dtypes)\n",
    "    df = df[df[\"mei\"] == \"S\"].copy()\n",
    "    df['ano_ini'] = (df['ini'] // 10000).fillna(0).astype(\"int16\")\n",
    "    df['ano_fim'] = (df['fim'] // 10000).fillna(0).astype(\"int16\")\n",
    "    return df.drop(columns=[\"mei\", \"ini\", \"fim\"])\n",
    "\n",
    "def classify_sector(df):\n",
    "    divisao = (df['cnae'] // 100000).fillna(0).astype(int)\n",
    "    df['setor'] = 'Servicos'\n",
    "    df.loc[divisao.between(1, 3), 'setor'] = 'Agro'\n",
    "    df.loc[divisao.between(5, 33), 'setor'] = 'Industria'\n",
    "    df.loc[divisao == 84, 'setor'] = 'Setor Publico'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1bcb8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎\u001b[0m|\n"
     ]
    }
   ],
   "source": [
    "import basedosdados as bd\n",
    "\n",
    "billing_id = \"dissertacao-pnate\"\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    dados.ano,\n",
    "    dados.sigla_uf,\n",
    "    dados.id_municipio,\n",
    "    diretorio_id_municipio.nome AS municipio_nome,\n",
    "    dic1.valor AS grau_instrucao_1985_2005,\n",
    "    dic2.valor AS grau_instrucao_apos_2005,\n",
    "    COUNT(*) AS total_vinculos\n",
    "FROM `basedosdados.br_me_rais.microdados_vinculos` AS dados TABLESAMPLE SYSTEM (5 PERCENT)\n",
    "LEFT JOIN `basedosdados.br_bd_diretorios_brasil.municipio` AS diretorio_id_municipio\n",
    "    ON dados.id_municipio = diretorio_id_municipio.id_municipio\n",
    "LEFT JOIN `basedosdados.br_me_rais.dicionario` AS dic1\n",
    "    ON dados.grau_instrucao_1985_2005 = dic1.chave \n",
    "    AND dic1.nome_coluna = 'grau_instrucao_1985_2005' \n",
    "    AND dic1.id_tabela = 'microdados_vinculos'\n",
    "LEFT JOIN `basedosdados.br_me_rais.dicionario` AS dic2\n",
    "    ON dados.grau_instrucao_apos_2005 = dic2.chave \n",
    "    AND dic2.nome_coluna = 'grau_instrucao_apos_2005' \n",
    "    AND dic2.id_tabela = 'microdados_vinculos'\n",
    "WHERE dados.ano >= 1990\n",
    "  AND dados.sigla_uf IN ('AL', 'BA', 'CE', 'MA', 'PB', 'PE', 'PI', 'RN', 'SE')\n",
    "GROUP BY 1, 2, 3, 4, 5, 6\n",
    "ORDER BY ano, id_municipio\n",
    "\"\"\"\n",
    "\n",
    "df = bd.read_sql(query=query, billing_project_id=billing_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4d4f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cria a coluna unificada pegando o valor que existir\n",
    "df['escolaridade'] = df['grau_instrucao_1985_2005'].fillna(df['grau_instrucao_apos_2005'])\n",
    "\n",
    "# 2. Remove as colunas antigas para limpar o df\n",
    "df.drop(columns=['grau_instrucao_1985_2005', 'grau_instrucao_apos_2005'], inplace=True)\n",
    "\n",
    "# 3. Agrupa novamente para consolidar as linhas (necessﾃ｡rio se os IDs eram diferentes)\n",
    "df = df.groupby(['ano', 'sigla_uf', 'id_municipio', 'municipio_nome', 'escolaridade'], as_index=False)['total_vinculos'].sum()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
